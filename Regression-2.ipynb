{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb6a8e2-142a-4cc9-ac0a-6dc54b5f2bb1",
   "metadata": {},
   "source": [
    "# Question 1: Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c723c-c136-4bab-b0d3-b8303198a481",
   "metadata": {},
   "source": [
    "# Ans\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3185c40-412d-4277-aa3c-231f160d4cc9",
   "metadata": {},
   "source": [
    "**R-squared (RÂ²)**, also known as the coefficient of determination, is a statistical measure used to assess the goodness of fit in linear regression models. It signifies the proportion of the variance in the dependent variable (Y) that is predictable from the independent variable (X). \n",
    "\n",
    "### Calculation of R-squared:\n",
    "\n",
    "1. **Total Sum of Squares (TSS)**: $( TSS = \\sum (Y - \\bar{Y})^2 $)\n",
    "   - $( Y $) represents the actual observed values.\n",
    "   - $( \\bar{Y} $) is the mean of the observed values.\n",
    "\n",
    "2. **Residual Sum of Squares (RSS)**: $( RSS = \\sum (Y - \\hat{Y})^2 $)\n",
    "   - $( \\hat{Y} $) represents the predicted values obtained from the regression model.\n",
    "\n",
    "3. **R-squared Formula**: $( R^2 = 1 - \\frac{RSS}{TSS} $)\n",
    "\n",
    "### Interpretation of R-squared:\n",
    "\n",
    "- **Range**: R-squared values range from 0 to 1.\n",
    "- **Interpretation**:\n",
    "  - An R-squared of 1 indicates that the regression model perfectly predicts the dependent variable.\n",
    "  - An R-squared of 0 means the model does not explain any of the variability in the dependent variable around its mean.\n",
    "\n",
    "### Significance of R-squared:\n",
    "\n",
    "- **Model Fit**: R-squared measures how well the regression model fits the observed data.\n",
    "- **Variance Explanation**: It indicates the proportion of variance in the dependent variable that is explained by the independent variable(s).\n",
    "\n",
    "### Limitations of R-squared:\n",
    "\n",
    "- **Context**: R-squared alone doesn't provide context. It does not indicate the correctness of the model's assumptions or the significance of the predictors.\n",
    "- **Multiple Models**: Comparing R-squared values between models with different predictors is unreliable.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "R-squared is a statistical measure that quantifies the proportion of variance in the dependent variable that is explained by the independent variable(s) in a linear regression model. It is a useful metric to evaluate the goodness of fit but should be interpreted in conjunction with other metrics and domain knowledge to draw meaningful conclusions about the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2265ae-1e6c-4dda-97f4-27798ed7dcfe",
   "metadata": {},
   "source": [
    "# Question 2 : Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6b799-9288-47bc-b2bc-2a32b5284e2a",
   "metadata": {},
   "source": [
    "## Ans\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7901ee-26e6-4e01-a1d3-171aa1508269",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified form of R-squared that accounts for the number of predictors in the model. It adjusts the R-squared value to compensate for the number of independent variables in the model. It aims to provide a more accurate measure of the goodness of fit without inflating the R-squared value when more predictors are added, which regular R-squared does not consider.\n",
    "\n",
    "### Calculation of Adjusted R-squared:\n",
    "\n",
    "Adjusted R-squared is calculated using the formula:\n",
    "\n",
    "$[ \\text{Adjusted R}^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - k - 1} $]\n",
    "\n",
    "Where:\n",
    "- \\( R^2 \\) is the regular R-squared.\n",
    "- \\( n \\) is the number of observations or data points.\n",
    "- \\( k \\) is the number of predictors (independent variables) in the model.\n",
    "\n",
    "### Differences between R-squared and Adjusted R-squared:\n",
    "\n",
    "1. **Compensation for Complexity**:\n",
    "    - R-squared increases as more predictors are added to the model, regardless of whether they improve prediction. Adjusted R-squared considers the number of predictors and increases only if additional predictors improve the model significantly.\n",
    "\n",
    "2. **Penalization for Extra Predictors**:\n",
    "    - R-squared does not penalize the addition of less influential predictors. Adjusted R-squared penalizes the addition of variables that do not significantly contribute to the model's prediction.\n",
    "\n",
    "3. **Range**:\n",
    "    - R-squared always increases or remains the same with the addition of predictors. Adjusted R-squared may increase or decrease, depending on whether the new predictors improve the model enough to offset the increased complexity.\n",
    "\n",
    "4. **Interpretation**:\n",
    "    - Higher R-squared values may falsely indicate a better fit due to the inclusion of more predictors. Adjusted R-squared offers a more accurate reflection of how well the model's predictors explain the variation in the dependent variable.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Adjusted R-squared is a modified version of R-squared that considers the number of predictors in the model, providing a more accurate assessment of the model's goodness of fit by penalizing the addition of less influential predictors. It serves as a more reliable metric for model evaluation when dealing with multiple predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d72f9b9-34f3-40dc-b4ba-e54dfea86ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (TV only): 0.8854008075872213\n",
      "Adjusted R-squared (TV only): 0.8842314280728052\n",
      "R-squared (TV and Radio): 0.9332412058539468\n",
      "Adjusted R-squared (TV and Radio): 0.9318647358715539\n",
      "R-squared (TV, Radio, Newspaper): 0.9423745391994103\n",
      "Adjusted R-squared (TV, Radio, Newspaper): 0.9405737435493918\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generating sample data for advertising budget and sales\n",
    "np.random.seed(42)\n",
    "ad_budget_tv = np.random.rand(100, 1) * 50  # TV advertising budget\n",
    "ad_budget_radio = np.random.rand(100, 1) * 30  # Radio advertising budget\n",
    "ad_budget_newspaper = np.random.rand(100, 1) * 20  # Newspaper advertising budget\n",
    "\n",
    "sales = 7 + 0.5 * ad_budget_tv + 0.2 * ad_budget_radio + 0.1 * ad_budget_newspaper + np.random.randn(100, 1) * 2\n",
    "\n",
    "# Fit a linear regression model with different numbers of predictors\n",
    "def calculate_r_squared(X, Y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, Y)\n",
    "    return model.score(X, Y)\n",
    "\n",
    "# R-squared for different predictor combinations\n",
    "r_squared_tv = calculate_r_squared(ad_budget_tv, sales)\n",
    "r_squared_tv_radio = calculate_r_squared(np.concatenate((ad_budget_tv, ad_budget_radio), axis=1), sales)\n",
    "r_squared_all = calculate_r_squared(np.concatenate((ad_budget_tv, ad_budget_radio, ad_budget_newspaper), axis=1), sales)\n",
    "\n",
    "# Calculate Adjusted R-squared\n",
    "n = len(sales)  # Number of observations\n",
    "k1 = 1  # Number of predictors (TV)\n",
    "k2 = 2  # Number of predictors (TV and Radio)\n",
    "k3 = 3  # Number of predictors (TV, Radio, and Newspaper)\n",
    "\n",
    "adjusted_r_squared_tv = 1 - (1 - r_squared_tv) * (n - 1) / (n - k1 - 1)\n",
    "adjusted_r_squared_tv_radio = 1 - (1 - r_squared_tv_radio) * (n - 1) / (n - k2 - 1)\n",
    "adjusted_r_squared_all = 1 - (1 - r_squared_all) * (n - 1) / (n - k3 - 1)\n",
    "\n",
    "print(\"R-squared (TV only):\", r_squared_tv)\n",
    "print(\"Adjusted R-squared (TV only):\", adjusted_r_squared_tv)\n",
    "print(\"R-squared (TV and Radio):\", r_squared_tv_radio)\n",
    "print(\"Adjusted R-squared (TV and Radio):\", adjusted_r_squared_tv_radio)\n",
    "print(\"R-squared (TV, Radio, Newspaper):\", r_squared_all)\n",
    "print(\"Adjusted R-squared (TV, Radio, Newspaper):\", adjusted_r_squared_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0144b5-fb97-4658-8ba2-75c36d1d187b",
   "metadata": {},
   "source": [
    "# Question 3 : When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b019857-39af-42fc-95be-a3663770cdc3",
   "metadata": {},
   "source": [
    "## Ans\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3741e-fd16-421a-819e-d2c045d29b2e",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate in situations where you are dealing with multiple predictors or independent variables in a regression model. It becomes especially useful when you want to compare the goodness of fit of models with varying numbers of predictors.\n",
    "\n",
    "### Situations where Adjusted R-squared is preferred:\n",
    "\n",
    "1. **Multiple Predictor Models**:\n",
    "   - When comparing models with different numbers of predictors, such as in feature selection or model comparison, Adjusted R-squared helps in understanding whether additional predictors significantly improve the model.\n",
    "\n",
    "2. **Avoiding Overfitting**:\n",
    "   - It helps in preventing overfitting by penalizing the addition of less influential predictors. As the number of predictors increases, regular R-squared tends to increase even with insignificant predictors, while Adjusted R-squared may decrease if the new predictors don't add substantial explanatory power.\n",
    "\n",
    "3. **Model Selection**:\n",
    "   - When choosing among various models, especially in cases where simplicity and interpretability are essential, Adjusted R-squared can assist in identifying the model with a balance between goodness of fit and complexity.\n",
    "\n",
    "4. **Comparing Complex Models**:\n",
    "   - In instances where you have multiple models with varying complexities and numbers of predictors, Adjusted R-squared is more useful as it considers the impact of additional predictors on model performance.\n",
    "\n",
    "5. **Regression Analysis with Large Number of Predictors**:\n",
    "   - In scenarios where the number of predictors is relatively large compared to the number of observations, Adjusted R-squared is a better measure to evaluate model fit.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Adjusted R-squared is more suitable in scenarios involving multiple predictors, where the goal is to evaluate model performance, compare models with different complexities, and prevent overfitting by penalizing the addition of less influential predictors. It provides a more accurate reflection of a model's goodness of fit by considering the number of predictors in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d3f30-c00e-496e-b0d0-aac0a9f7e96c",
   "metadata": {},
   "source": [
    "# Question 5 : Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d47cf-dc79-45a6-9148-ace344e8dc5a",
   "metadata": {},
   "source": [
    "# Ans \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873dc950-2dcc-4b82-9e62-9fb1bc5b6683",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages of Regression Evaluation Metrics:\n",
    "\n",
    "### Mean Squared Error (MSE):\n",
    "- **MSE and RMSE** are suitable for scenarios where larger errors need to be penalized, such as in applications where a few larger errors are critical.\n",
    "#### Advantages:\n",
    "- **Sensitive to Errors**: MSE penalizes larger errors due to the squaring of residuals. It prioritizes minimizing larger errors, making it suitable for applications where larger errors are more critical.\n",
    "- **Differentiable**: Since it is differentiable, MSE is suitable for applications in optimization algorithms.\n",
    "\n",
    "#### Disadvantages:\n",
    "- **Units of Measurement**: MSE is not in the same units as the target variable, making direct interpretation challenging.\n",
    "- **Sensitivity to Outliers**: Squaring residuals gives higher weight to outliers, which might not always reflect the model's performance accurately in real-world scenarios.\n",
    "\n",
    "### Root Mean Squared Error (RMSE):\n",
    "\n",
    "#### Advantages:\n",
    "- **Interpretable**: RMSE is in the same units as the target variable, making it more interpretable and easily relatable to the actual target variable.\n",
    "- **Sensitivity to Large Errors**: Similar to MSE, RMSE is sensitive to larger errors and can penalize them.\n",
    "\n",
    "#### Disadvantages:\n",
    "- **Outlier Sensitivity**: RMSE, like MSE, is sensitive to outliers, which might not reflect the overall model performance accurately, especially in real-world scenarios with noisy data.\n",
    "\n",
    "### Mean Absolute Error (MAE):\n",
    "- **MAE** is more robust to outliers and provides a more balanced evaluation of the model's performance in scenarios where larger errors are not significantly more critical than smaller ones.\n",
    "\n",
    "#### Advantages:\n",
    "- **Robust to Outliers**: MAE is less sensitive to outliers as it computes the average of absolute differences.\n",
    "- **Interpretability**: Similar to RMSE, MAE is in the same units as the target variable, making it easy to interpret and compare directly to the actual values.\n",
    "\n",
    "#### Disadvantages:\n",
    "- **Not Sensitive to Error Magnitude**: MAE treats all errors equally and doesn't differentiate between larger and smaller errors, potentially neglecting the importance of larger errors in certain applications.\n",
    "\n",
    "\n",
    "\n",
    "Choosing the appropriate evaluation metric in regression analysis depends on the specific objectives of the problem, the context of the data, and the impact of different types of errors on the final model performance.\n",
    "\n",
    "- All three metrics have their strengths and weaknesses, and the choice of which to use depends on the specific requirements of the problem, the impact of outliers, and the nature of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "560fc919-1116-482a-90b3-f51a0d9a57e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 6828.571428571428\n",
      "RMSE: 82.63517065131184\n",
      "MAE: 57.142857142857146\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Actual and predicted house prices\n",
    "actual = np.array([300, 400, 250, 600, 700,750,800])\n",
    "predicted = np.array([320, 380, 290, 550, 680,800,1000])\n",
    "\n",
    "# Calculating MSE, RMSE, and MAE\n",
    "mse = np.mean((actual - predicted) ** 2)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(actual - predicted))\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c5bd3-53f7-42f2-9448-3d233a21338e",
   "metadata": {},
   "source": [
    "# Question 6 : Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08296475-2816-4d67-b494-4c188007eae6",
   "metadata": {},
   "source": [
    "# Ans\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ebe104-e993-4703-9e95-8fd7c4c20a4f",
   "metadata": {},
   "source": [
    "**Lasso (Least Absolute Shrinkage and Selection Operator)** regularization is a technique used in linear regression to prevent overfitting by adding a penalty term to the regression equation. The penalty term is the absolute sum of the coefficients multiplied by a constant alpha. Lasso aims to shrink the coefficients of less important features to zero, effectively performing feature selection.\n",
    "\n",
    "### Differences between Lasso and Ridge Regularization:\n",
    "\n",
    "#### Penalty Term:\n",
    "- **Lasso** uses the L1 norm penalty, which is the absolute sum of the coefficients: \\(\\alpha \\times \\sum{|w_i|}\\), where \\(w_i\\) are the coefficients.\n",
    "- **Ridge** uses the L2 norm penalty, which is the sum of the squares of the coefficients: \\(\\alpha \\times \\sum{w_i^2}\\).\n",
    "\n",
    "#### Effect on Coefficients:\n",
    "- **Lasso** has the property of setting some coefficients to exactly zero, effectively performing feature selection by eliminating less important features.\n",
    "- **Ridge** tends to shrink coefficients but rarely reduces them to zero, keeping all features but penalizing their magnitude.\n",
    "\n",
    "#### Nature of Solution:\n",
    "- **Lasso** provides sparse solutions by selecting a subset of features and setting others to zero.\n",
    "- **Ridge** tends to provide a solution with small coefficients but does not enforce sparsity.\n",
    "\n",
    "### Appropriate Use Cases:\n",
    "\n",
    "- **When Feature Selection is Required**: Lasso is more appropriate when feature selection is desired or when dealing with a large number of features, as it automatically performs variable selection by setting less important feature coefficients to zero.\n",
    "- **Dealing with a Sparse Model**: When the problem domain is expected to have only a few important features contributing to the target variable, Lasso regularization is preferred due to its tendency to provide a sparse model.\n",
    "- **Handling Multicollinearity**: When dealing with multicollinearity, Ridge is usually preferred over Lasso. However, if feature selection is necessary in the presence of multicollinearity, elastic net regularization, which combines Lasso and Ridge, can be a suitable choice.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Lasso regularization is suitable when feature selection is crucial, and a sparse model with only a subset of features contributing significantly to the outcome is desired. Ridge, on the other hand, is preferred when handling multicollinearity or when all features might play a role in the prediction without complete elimination. The choice between Lasso and Ridge depends on the nature of the problem, the significance of feature selection, and the trade-offs between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a23237-b86e-4da9-9d23-2bf10cb1a6bd",
   "metadata": {},
   "source": [
    "# predicting housing prices using Lasso and Ridge regularization.\n",
    "-Lasso and Ridge regressions to predict house prices based on features such as median income, housing median age, average rooms, average bedrooms, population, etc.\n",
    "\n",
    "\n",
    "lasso and Ridge regressions are applied to predict house prices. The count of non-zero coefficients for Lasso represents the selected features, whereas for Ridge, it shows the number of non-zero coefficients. This demonstrates how Lasso performs feature selection by reducing some coefficients to zero and how Ridge maintains non-zero coefficients but shrinks them to control model complexity.\n",
    "\n",
    "**Now, let's use Lasso regularization to perform feature selection:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8eb13736-8412-4a2c-bfdf-cc4a3865ab5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso - Selected features: ['MedInc', 'HouseAge', 'Latitude']\n",
      "Ridge - Non-zero coefficients:\n",
      "MedInc: 0.8544\n",
      "HouseAge: 0.1226\n",
      "AveRooms: -0.2944\n",
      "AveBedrms: 0.3392\n",
      "Population: -0.0023\n",
      "AveOccup: -0.0408\n",
      "Latitude: -0.8969\n",
      "Longitude: -0.8698\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load California housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply Lasso regression\n",
    "alpha_lasso = 0.1  # Lasso regularization parameter\n",
    "lasso = Lasso(alpha=alpha_lasso)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Retrieve selected features for Lasso\n",
    "selected_features_lasso = [feature_names[i] for i, coef in enumerate(lasso.coef_) if coef != 0]\n",
    "print(\"Lasso - Selected features:\", selected_features_lasso)\n",
    "\n",
    "# Apply Ridge regression\n",
    "alpha_ridge = 0.1  # Ridge regularization parameter\n",
    "ridge = Ridge(alpha=alpha_ridge)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Retrieve non-zero coefficients for Ridge\n",
    "non_zero_coeffs_ridge = {feature_names[i]: coef for i, coef in enumerate(ridge.coef_) if coef != 0}\n",
    "print(\"Ridge - Non-zero coefficients:\")\n",
    "for feature, coef in non_zero_coeffs_ridge.items():\n",
    "    print(f\"{feature}: {coef:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dea32180-ccbc-4778-837e-49fd04e44bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.02381</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.97188</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127    1.02381       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137    0.97188      2401.0  2.109842     37.86   \n",
       "\n",
       "   Longitude  Target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data ko DataFrame me convert karna\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Target variable (house prices) ko add karna\n",
    "df['Target'] = data.target\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73133bd3-9544-47c6-a295-06981ac026ab",
   "metadata": {},
   "source": [
    "# Question 7 : How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce388a3-5b44-40e6-b683-dcd16e39d955",
   "metadata": {},
   "source": [
    "## Ans\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2e1715-a41d-4ecd-ae1c-97d3b276330f",
   "metadata": {},
   "source": [
    "Regularized linear models are used to prevent overfitting by introducing a penalty term in the model's cost function. This penalty discourages overly complex models by imposing constraints on the coefficients, thus preventing them from reaching extreme values.\n",
    "\n",
    "### How Regularized Linear Models Prevent Overfitting:\n",
    "\n",
    "1. **Penalizing Complexity**:\n",
    "   - Regularization methods (e.g., Lasso, Ridge) add a penalty term to the ordinary least squares (OLS) or linear regression objective function. This penalty is based on the magnitude of the coefficients.\n",
    "   \n",
    "2. **Controlling Coefficients**:\n",
    "   - By penalizing the coefficients, these methods either reduce the coefficient values (Ridge) or force some coefficients to zero (Lasso).\n",
    "   \n",
    "3. **Simplifying the Model**:\n",
    "   - As a result, these models become less sensitive to individual data points, outliers, or noise in the training data, leading to a simpler and more generalizable model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0dee3ce-124c-457e-8399-668b026c5918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Train Score (R^2): 0.778\n",
      "Ridge Test Score (R^2): 0.760\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Ridge regularization\n",
    "alpha = 1.0  # Regularization parameter\n",
    "ridge = Ridge(alpha=alpha)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_score = ridge.score(X_train, y_train)\n",
    "test_score = ridge.score(X_test, y_test)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = ridge.predict(X_train)\n",
    "y_pred_test = ridge.predict(X_test)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Ridge Train Score (R^2): {r2_train:.3f}\")\n",
    "print(f\"Ridge Test Score (R^2): {r2_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b887bf3a-8182-423e-bc92-089b8f2e4ee9",
   "metadata": {},
   "source": [
    "how Ridge regularization helps prevent overfitting in a linear regression model using a sample dataset.\n",
    "In this example, we use a sample dataset to fit a Ridge regression model. The regularization parameter (`alpha`) is set to `1.0` for illustration purposes. The model's performance on both the training and testing data is then evaluated using the `score` method. A lower difference between the train and test scores indicates the prevention of overfitting due to the regularization applied. Adjusting the `alpha` parameter helps control the extent of regularization and thus the prevention of overfitting in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe1bfa1-f214-4df3-a1ae-55bd7c5ae21a",
   "metadata": {},
   "source": [
    "# Question 8 : Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3900afcc-8bc4-4aa8-b955-36e756b8ca98",
   "metadata": {},
   "source": [
    "# Ans\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dcf7c5-1bf3-408d-9014-ad7b96c49094",
   "metadata": {},
   "source": [
    "Regularized linear models like Ridge, Lasso, and Elastic Net are valuable tools in regression analysis, but they have certain limitations that make them not always the best choice for every scenario:\n",
    "\n",
    "### Limitations of Regularized Linear Models:\n",
    "\n",
    "1. **Variable Selection Bias**:\n",
    "   - **Lasso**: Although Lasso can perform feature selection by zeroing out coefficients, it might exhibit variable selection bias in the presence of correlated predictors. It may arbitrarily select one variable over another due to randomness.\n",
    "  \n",
    "2. **Sensitivity to Hyperparameters**:\n",
    "   - Regularization methods like Ridge and Lasso require setting hyperparameters (e.g., alpha) which might be sensitive to the choice of values. Selecting an inappropriate value can impact the model's performance.\n",
    "\n",
    "3. **Assumption of Linearity**:\n",
    "   - Regularized linear models assume a linear relationship between features and the target variable. If the relationship is nonlinear, these models might not capture the complexities adequately.\n",
    "\n",
    "4. **Inadequate with High Dimensionality**:\n",
    "   - When dealing with high-dimensional datasets, the effectiveness of regularization in handling overfitting might diminish. In these cases, other models or feature selection techniques might be more suitable.\n",
    "\n",
    "5. **Impact of Outliers**:\n",
    "   - Outliers might still influence regularized linear models, especially in Lasso where outliers can have a more pronounced effect due to the absolute penalty on coefficients.\n",
    "\n",
    "6. **Sensitivity to Scaling**:\n",
    "   - Regularization methods are sensitive to feature scaling. If the features are not on the same scale, it might affect the regularization effect.\n",
    "\n",
    "### Cases Where Regularized Linear Models May Not Be Ideal:\n",
    "\n",
    "1. **Non-Linear Relationships**:\n",
    "   - When the relationship between predictors and the target variable is inherently non-linear, other non-linear models might be more appropriate, such as decision trees, random forests, or neural networks.\n",
    "\n",
    "2. **When Interpretability is Critical**:\n",
    "   - In scenarios where interpretability is crucial and a sparse solution is not necessary, simpler linear models might be more suitable.\n",
    "\n",
    "3. **Feature Importance**:\n",
    "   - If understanding feature importance is a priority, other methods like decision trees or ensemble methods provide more straightforward feature importance measures.\n",
    "\n",
    "4. **Highly Correlated Predictors**:\n",
    "   - Regularized linear models might struggle in scenarios with highly correlated predictors as they might arbitrarily select one predictor over the other.\n",
    "\n",
    "5. **Sparse Data**:\n",
    "   - In the case of sparse data where there are too few observations compared to the number of predictors, regularization might not be as effective.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Regularized linear models are powerful tools, but their effectiveness depends on the nature of the problem, the dataset, and the specific goals of the analysis. In situations where assumptions are violated, interpretability is crucial, or the relationship is non-linear, other models or techniques might be more appropriate for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08928083-3ff3-473f-a8a1-d39956799c67",
   "metadata": {},
   "source": [
    "# Question 9 : You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metricQuestion 9 : You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a6216-34d1-4d46-bede-f642b7242971",
   "metadata": {},
   "source": [
    "# Ans\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3e15e-ec4e-4323-91bb-b5d40a387c12",
   "metadata": {},
   "source": [
    "Choosing the better model between Model A and Model B based solely on their evaluation metrics (RMSE of 10 for Model A and MAE of 8 for Model B) can depend on the specific context and the goal of the analysis. However, there are some considerations to keep in mind:\n",
    "\n",
    "### Comparison of RMSE and MAE:\n",
    "\n",
    "- **RMSE (Root Mean Squared Error)**:\n",
    "  - RMSE considers both the magnitude of errors and penalizes larger errors more due to the squaring of residuals.\n",
    "  - A lower RMSE indicates better accuracy in predicting the target variable.\n",
    "  \n",
    "- **MAE (Mean Absolute Error)**:\n",
    "  - MAE treats all errors equally and is not sensitive to the magnitude of errors.\n",
    "  - A lower MAE also signifies better accuracy in prediction but does not penalize larger errors as significantly as RMSE.\n",
    "\n",
    "### Choice between RMSE and MAE:\n",
    "\n",
    "- **RMSE's Sensitivity to Outliers**:\n",
    "  - If the dataset contains outliers and the goal is to penalize larger errors more, RMSE might be more appropriate.\n",
    "  \n",
    "- **Robustness of MAE**:\n",
    "  - If outliers are a concern and it's important to have a more robust evaluation metric less sensitive to outliers, MAE might be preferred.\n",
    "\n",
    "### Limitations of Evaluation Metrics:\n",
    "\n",
    "- **RMSE and Outliers**:\n",
    "  - RMSE can be heavily influenced by outliers due to squaring, whereas MAE is more robust against outliers.\n",
    "\n",
    "- **Interpretability**:\n",
    "  - RMSE is less interpretable since it squares the errors, making it harder to relate to the original units of the target variable.\n",
    "\n",
    "- **Relative Comparison**:\n",
    "  - The relative difference between RMSE and MAE depends on the specific dataset, and the choice of metric might vary based on the context of the problem.\n",
    "\n",
    "### Decision Making:\n",
    "\n",
    "- If the goal is to penalize larger errors more, the RMSE of Model A being 10 might suggest that it performs better in capturing and penalizing the larger errors.\n",
    "- However, if robustness against outliers is more critical or if interpretability is a concern, the MAE of 8 from Model B might indicate better performance.\n",
    "\n",
    "Ultimately, the choice between RMSE and MAE should be guided by the problem context and the specific requirements of the analysis, keeping in mind the metric's limitations and the goals of the modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3046b5b-1536-49fa-81af-2f56f3841108",
   "metadata": {},
   "source": [
    "# Question 10 : You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c231937-d20d-447c-8339-171b603aa381",
   "metadata": {},
   "source": [
    "# Ans\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4c924-57f2-411d-a385-650ee960236d",
   "metadata": {},
   "source": [
    "Choosing the better performer between Model A (Ridge regularization with a parameter of 0.1) and Model B (Lasso regularization with a parameter of 0.5) can depend on several factors, including the context of the problem and specific trade-offs associated with each type of regularization:\n",
    "\n",
    "### Ridge vs. Lasso Regularization:\n",
    "\n",
    "- **Ridge Regularization**:\n",
    "  - Reduces the size of coefficients and shrinks them towards zero, but they never become exactly zero.\n",
    "  - Generally better at handling multicollinearity.\n",
    "  - Suitable for scenarios where all features might contribute to the prediction, but with reduced impact.\n",
    "\n",
    "- **Lasso Regularization**:\n",
    "  - Performs both coefficient shrinkage and feature selection, forcing some coefficients to zero.\n",
    "  - Ideal for situations where feature selection is crucial, as it provides a sparse solution by eliminating less important features.\n",
    "\n",
    "### Trade-offs and Limitations:\n",
    "\n",
    "- **Ridge Limitations**:\n",
    "  - Ridge might not perform feature selection effectively; it keeps all features, though with reduced coefficients.\n",
    "  - Not suitable for scenarios requiring sparse solutions or when interpretability and feature selection are crucial.\n",
    "\n",
    "- **Lasso Limitations**:\n",
    "  - Lasso's variable selection might be sensitive to correlated predictors, arbitrarily selecting one variable over another due to randomness (variable selection bias).\n",
    "  - In situations with high dimensionality or multicollinearity, Lasso might not perform as effectively.\n",
    "\n",
    "### Model Comparison:\n",
    "\n",
    "- **Model A (Ridge)**:\n",
    "  - Ridge with a regularization parameter of 0.1 might provide a balanced reduction in coefficients without eliminating any. This could be preferable if maintaining most features is crucial, and multicollinearity is a concern.\n",
    "\n",
    "- **Model B (Lasso)**:\n",
    "  - Lasso with a regularization parameter of 0.5 might be better if sparsity or feature selection is vital, as it eliminates some features by setting their coefficients to zero.\n",
    "\n",
    "### Decision Making:\n",
    "\n",
    "- If interpretability and retaining most features are significant, Model A (Ridge) might be more appropriate.\n",
    "- However, if feature selection and sparsity are crucial, Model B (Lasso) might be the better choice.\n",
    "\n",
    "Ultimately, the selection of the better performer between Ridge and Lasso regularization depends on the specific requirements of the analysis, the nature of the dataset, and the goals of the modeling task. The decision should consider the trade-offs and limitations associated with each regularization method and align with the specific needs of the problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
